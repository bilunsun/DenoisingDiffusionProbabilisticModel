defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .


lit_model:
  _target_: src.lit_model.BaseModel 

  models_config:
    
    # eps_model:
    #   _target_: src.utils.models.SmolUNet
    #   in_channels: 3 
    #   out_channels: [64, 128, 256, 512, 1024]

    eps_model:
      _target_: src.utils.vit.ViT
      image_size: ${datamodule.dataset_config.resize}
      patch_size: 4 
      d_model: 128 
      depth: 6 
      n_heads: 4 
      ffwd_dim: 1024
      n_channels: 1
      dim_head: 64 
      dropout: 0.0

    # eps_model:
    #   _target_: src.utils.vit.ViT
    #   image_size: ${datamodule.dataset_config.resize}
    #   patch_size: 8
    #   d_model: 768 
    #   depth: 12
    #   n_heads: 12
    #   ffwd_dim: 2048 
    #   n_channels: 3
    #   dim_head: 64 
    #   dropout: 0.0

  optimizer_config:
    _target_: torch.optim.Adam
    lr: 1e-4

  diffusion_config:
    _target_: src.utils.ddpm.Diffusion

    schedule_config:
      _target_: src.utils.ddpm.Diffusion._cosine_beta_schedule
      s: 0.008 
      T: 200 
    # schedule_config:
    #   _target_: src.utils.ddpm.Diffusion._linear_beta_schedule
    #   beta_1: 0.0001
    #   beta_T: 0.02
    #   T: 200

  scheduler_config: 
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${trainer.max_epochs}

  use_weights_path: null


callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val_loss


logger:
  _target_: pytorch_lightning.loggers.wandb.WandbLogger
  project: TEST-ddpm


datamodule:
  _target_: src.utils.data.DataModule

  # dataset_config:
  #   _target_: src.utils.data.get_image_folder_dataset
  #   resize: 32 
  #   data_dir: data
  #   image_dir: PokemonDataLarge
  #   val_len: 50 

  # dataset_config:
  #   _target_: src.utils.data.get_celebA_dataset
  #   resize: 32 
  #   data_dir: data
  #   val_len: 50 

  dataset_config:
    _target_: src.utils.data.get_mnist_dataset
    resize: 28 
    data_dir: data

  batch_size: 128 
  shuffle: True
  num_workers: 8


trainer:
  _target_: pytorch_lightning.Trainer
  deterministic: true
  accelerator: gpu
  devices: [0]
  max_epochs: 200 
  limit_val_batches: 10
  # val_check_interval: 1000 
  precision: 16


# Restart training from checkpoint from PyTorch Lightning
ckpt_path: null

# Reproducibility
seed: 123456
