defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .


lit_model:
  _target_: src.lit_model.BaseModel 

  models_config:
    
    eps_model:
      _target_: src.utils.models.SmolUNet
      in_channels: 3
      out_channels: [64, 128, 256, 512]

  optimizer_config:
    _target_: torch.optim.Adam
    lr: 2e-5

  diffusion_config:
    _target_: src.utils.ddpm.Diffusion

    schedule_config:
      _target_: src.utils.ddpm.Diffusion._cosine_beta_schedule
      s: 0.008 
      T: 200

  scheduler_config: 
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${trainer.max_epochs}

  use_weights_path: null


callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val_loss


logger:
  _target_: pytorch_lightning.loggers.wandb.WandbLogger
  project: TEST-ddpm


datamodule:
  _target_: src.utils.data.DataModule

  # dataset_config:
  #   _target_: src.utils.data.get_image_folder_dataset
  #   resize: 32 
  #   data_dir: data
  #   image_dir: PokemonDataLarge
  #   val_len: 50 

  dataset_config:
    _target_: src.utils.data.get_celebA_dataset
    resize: 32
    data_dir: data
    val_len: 50 

  batch_size: 64
  shuffle: True
  num_workers: 8


trainer:
  _target_: pytorch_lightning.Trainer
  deterministic: true
  accelerator: gpu
  devices: [0]
  max_epochs: 200 
  limit_val_batches: 10
  val_check_interval: 1000
  precision: 16


# Restart training from checkpoint from PyTorch Lightning
ckpt_path: null

# Reproducibility
seed: 123456
