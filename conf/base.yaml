defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .


lit_model:
  _target_: src.lit_model.BaseModel 

  models_config:
    # eps_model:
    #   _target_: src.utils.unet.UNet
    #   image_channels: 1
    eps_model:
      _target_: src.utils.models.SmolUNet
      in_channels: 1
      out_channels: [64, 128, 256, 512]

    # eps_model:
    #   _target_: src.utils.models.TestModel
    #   width: 28
    #   height: 28
    #   n_channels: 1
    #   T: ${lit_model.diffusion_config.T}

  optimizer_config:
    _target_: torch.optim.Adam
    lr: 2e-4

  diffusion_config:
    _target_: src.utils.ddpm.Diffusion
    beta_range: [0.0001, 0.02]
    T: 1000 

  scheduler_config: 
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${trainer.max_epochs}
    # T_max: 20000 

  use_weights_path: null


callbacks: null


logger:
  _target_: pytorch_lightning.loggers.wandb.WandbLogger
  project: TEST-ddpm


datamodule:
  _target_: src.utils.data.MNISTDataModule
  resize: 32
  batch_size: 32 
  data_dir: data
  shuffle: True
  num_workers: 8


trainer:
  _target_: pytorch_lightning.Trainer
  deterministic: true
  accelerator: gpu
  devices: [0]
  max_epochs: 50 
  val_check_interval: 1000 
  limit_val_batches: 50
  precision: 16


# Restart training from checkpoint from PyTorch Lightning
ckpt_path: null

# Reproducibility
seed: 123456
